# LLM Evaluation Suite

This project demonstrates a basic evaluation framework for testing reasoning quality,
instruction-following, and reliability of large language models.

## What this project does
- Evaluates model answers against ground truth
- Checks for reasoning explanations
- Flags uncertainty and weak responses

## Why this matters
In production AI systems, incorrect reasoning and hallucinations can cause serious issues.
This project shows how to systematically detect such failures.
